# æ¨¡å‹è®­ç»ƒè„šæœ¬

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import os
import sys
import time
import argparse
import numpy as np
import json
from datetime import datetime

# é¿å…é‡å¤å¯¼å…¥æç¤º
_import_messages = set()

def print_once(message):
    """ä»…æ‰“å°ä¸€æ¬¡æ¶ˆæ¯ï¼Œé¿å…é‡å¤"""
    if message not in _import_messages:
        print(message)
        _import_messages.add(message)

def get_lr(optimizer):
    """è·å–ä¼˜åŒ–å™¨çš„å½“å‰å­¦ä¹ ç‡"""
    try:
        return optimizer.param_groups[0]['lr']
    except (IndexError, KeyError, AttributeError):
        return 0.0

# å°è¯•å¯¼å…¥tqdmï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›ä¸€ä¸ªç®€å•çš„æ›¿ä»£å®ç°
try:
    from tqdm import tqdm
    HAS_TQDM = True
    print_once("âœ… æˆåŠŸå¯¼å…¥tqdmè¿›åº¦æ¡")
except ImportError:
    HAS_TQDM = False
    print_once("âš ï¸ æ— æ³•å¯¼å…¥tqdmï¼Œå°†ä½¿ç”¨ç®€å•è¿›åº¦æ˜¾ç¤º")
    # å®šä¹‰ä¸€ä¸ªç®€å•çš„tqdmæ›¿ä»£ç±»
    class SimpleProgressBar:
        def __init__(self, iterable=None, desc="", total=None):
            self.iterable = iterable
            self.desc = desc
            self.total = total if total is not None else len(iterable) if iterable is not None else 0
            self.current = 0
            self.start_time = time.time()
        
        def __iter__(self):
            if self.iterable is None:
                raise ValueError("No iterable provided")
            
            for item in self.iterable:
                yield item
                self.current += 1
                self._update_display()
        
        def __len__(self):
            # å®ç°__len__æ–¹æ³•ä»¥æ”¯æŒlen(progress_bar)
            return self.total
        
        def _update_display(self):
            elapsed = time.time() - self.start_time
            if elapsed > 0:
                rate = self.current / elapsed
                eta = (self.total - self.current) / rate if rate > 0 else 0
                progress = self.current / self.total * 100 if self.total > 0 else 0
                sys.stdout.write(f'\r{self.desc} {self.current}/{self.total} ({progress:.1f}%) ETA: {eta:.1f}s')
                sys.stdout.flush()
        
        def set_postfix(self, *args, **kwargs):
            # æ¥å—ä½ç½®å‚æ•°ï¼ˆå¦‚å­—å…¸ï¼‰å’Œå…³é”®å­—å‚æ•°
            # å­˜å‚¨postfixä¿¡æ¯ä»¥å¤‡å¯èƒ½çš„ä½¿ç”¨
            if args and isinstance(args[0], dict):
                self.postfix = args[0]
            else:
                self.postfix = kwargs
            pass
    
    # æ›¿æ¢tqdm
    tqdm = SimpleProgressBar

# å°è¯•å¯¼å…¥TensorBoardï¼Œå¦‚æœå¤±è´¥åˆ™è®¾ç½®ä¸ºNone
try:
    from torch.utils.tensorboard import SummaryWriter
    HAS_TENSORBOARD = True
    print_once("âœ… æˆåŠŸå¯¼å…¥TensorBoard")
except ImportError:
    SummaryWriter = None
    HAS_TENSORBOARD = False
    print_once("âš ï¸ æ— æ³•å¯¼å…¥TensorBoardï¼Œå°†è·³è¿‡TensorBoardè®°å½•åŠŸèƒ½")


# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from model_def import get_model, count_parameters
from data_loader import MNISTDataLoader, DEFAULT_DATA_CONFIG


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(description='MNIST æ‰‹å†™æ•°å­—è¯†åˆ«æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--model', type=str, default='medium', choices=['simple', 'medium', 'advanced'],
                      help='é€‰æ‹©æ¨¡å‹ç±»å‹')
    parser.add_argument('--epochs', type=int, default=30, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹é‡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001, help='åˆå§‹å­¦ä¹ ç‡')
    parser.add_argument('--weight-decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    parser.add_argument('--val-ratio', type=float, default=0.1, help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--patience', type=int, default=5, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--save-dir', type=str, default='./models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--log-dir', type=str, default='./logs', help='æ—¥å¿—ä¿å­˜ç›®å½•')
    parser.add_argument('--resume', type=str, default=None, help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--use-mixed-precision', action='store_true', help='ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    parser.add_argument('--gradient-accumulation-steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--warmup-epochs', type=int, default=2, help='å­¦ä¹ ç‡é¢„çƒ­è½®æ•°')
    parser.add_argument('--use-cosine-lr-scheduler', action='store_true', help='ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨')
    
    return parser.parse_args()


def get_lr(optimizer):
    """
    è·å–å½“å‰å­¦ä¹ ç‡
    """
    for param_group in optimizer.param_groups:
        return param_group['lr']


def create_optimizer(model, lr, weight_decay):
    """
    åˆ›å»ºä¼˜åŒ–å™¨
    """
    # ä½¿ç”¨Adamä¼˜åŒ–å™¨
    optimizer = optim.Adam(
        model.parameters(),
        lr=lr,
        weight_decay=weight_decay
    )
    return optimizer


def create_lr_scheduler(optimizer, config, total_steps):
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    if config['use_cosine_lr_scheduler']:
        # ä½™å¼¦é€€ç«è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=total_steps,
            eta_min=config['lr'] * 0.01  # æœ€å°å­¦ä¹ ç‡
        )
    else:
        # é˜¶æ¢¯å¼è¡°å‡è°ƒåº¦å™¨
        # ç¡®ä¿step_sizeè‡³å°‘ä¸º1ï¼Œé¿å…é™¤é›¶é”™è¯¯
        step_size = max(1, config['epochs'] // 3)
        scheduler = optim.lr_scheduler.StepLR(
            optimizer,
            step_size=step_size,
            gamma=0.1
        )
    
    return scheduler


def train_one_epoch(model, train_loader, criterion, optimizer, scaler, config, device, epoch, writer):
    """
    è®­ç»ƒä¸€ä¸ªè½®æ¬¡
    """
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    # æ¢¯åº¦ç´¯ç§¯è®¡æ•°å™¨
    accumulation_step = 0
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}/{config["epochs"]}')
    
    for i, (images, labels) in enumerate(progress_bar):
        accumulation_step += 1
        
        # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)
        
        # å‰å‘ä¼ æ’­ - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config['use_mixed_precision']:
            with autocast():
                output = model(images)
                loss = criterion(output, labels) / config['gradient_accumulation_steps']
        else:
            output = model(images)
            loss = criterion(output, labels) / config['gradient_accumulation_steps']
        
        # åå‘ä¼ æ’­å¹¶æ›´æ–°æ¢¯åº¦
        if config['use_mixed_precision']:
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦ç´¯ç§¯è¾¾åˆ°æŒ‡å®šæ­¥æ•°åæ›´æ–°å‚æ•°
            if accumulation_step % config['gradient_accumulation_steps'] == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)  # æ›´é«˜æ•ˆçš„æ¢¯åº¦æ¸…é›¶
        else:
            loss.backward()
            
            # æ¢¯åº¦ç´¯ç§¯è¾¾åˆ°æŒ‡å®šæ­¥æ•°åæ›´æ–°å‚æ•°
            if accumulation_step % config['gradient_accumulation_steps'] == 0:
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
        
        # ç»Ÿè®¡æŸå¤±å’Œå‡†ç¡®ç‡
        running_loss += loss.item() * config['gradient_accumulation_steps']  # æ¢å¤åŸå§‹æŸå¤±å€¼
        _, predicted = output.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # æ›´æ–°è¿›åº¦æ¡
        if HAS_TQDM:
            progress_bar.set_postfix({
                'loss': f'{running_loss/(i+1):.3f}',
                'acc': f'{100.*correct/total:.2f}%',
                'lr': f'{get_lr(optimizer):.6f}'
            })
        else:
            progress_bar.set_postfix(loss=f'{running_loss/(i+1):.3f}', 
                                   acc=f'{100.*correct/total:.2f}%',
                                   lr=f'{get_lr(optimizer):.6f}')
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    train_loss = running_loss / len(train_loader)
    train_acc = 100. * correct / total
    
    # è®°å½•åˆ°TensorBoardï¼ˆæ£€æŸ¥writeræ˜¯å¦ä¸ºNoneï¼‰
    if writer is not None and HAS_TENSORBOARD:
        try:
            writer.add_scalar('train/loss', train_loss, epoch)
            writer.add_scalar('train/accuracy', train_acc, epoch)
            writer.add_scalar('train/learning_rate', get_lr(optimizer), epoch)
        except Exception as e:
            print_once(f"âš ï¸ TensorBoardè®°å½•å¤±è´¥: {e}")
    elif not HAS_TENSORBOARD:
        print_once("âš ï¸ è·³è¿‡TensorBoardè®°å½•ï¼ˆTensorBoardä¸å¯ç”¨ï¼‰")
    else:
        print_once("âš ï¸ è·³è¿‡TensorBoardè®°å½•ï¼ˆwriterä¸ºNoneï¼‰")
    
    print(f"ğŸ“Š Epoch {epoch}/{config['epochs']} | è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}% | å­¦ä¹ ç‡: {get_lr(optimizer):.6f}")
    
    return train_loss, train_acc


def validate(model, val_loader, criterion, device, epoch, writer):
    """
    éªŒè¯æ¨¡å‹æ€§èƒ½
    """
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    
    # ä¸è®¡ç®—æ¢¯åº¦
    with torch.no_grad():
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        progress_bar = tqdm(val_loader, desc=f'Validation')
        
        for images, labels in progress_bar:
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)
            
            # å‰å‘ä¼ æ’­
            output = model(images)
            loss = criterion(output, labels)
            
            # ç»Ÿè®¡æŸå¤±å’Œå‡†ç¡®ç‡
            val_loss += loss.item()
            _, predicted = output.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            if HAS_TQDM:
                progress_bar.set_postfix({
                    'loss': f'{val_loss/len(progress_bar):.3f}',
                    'acc': f'{100.*correct/total:.2f}%'
                })
            else:
                progress_bar.set_postfix(loss=f'{val_loss/len(progress_bar):.3f}',
                                       acc=f'{100.*correct/total:.2f}%')
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    val_loss = val_loss / len(val_loader)
    val_acc = 100. * correct / total
    
    # è®°å½•åˆ°TensorBoardï¼ˆæ£€æŸ¥writeræ˜¯å¦ä¸ºNoneï¼‰
    if writer is not None and HAS_TENSORBOARD:
        try:
            writer.add_scalar('val/loss', val_loss, epoch)
            writer.add_scalar('val/accuracy', val_acc, epoch)
        except Exception as e:
            print_once(f"âš ï¸ TensorBoardè®°å½•å¤±è´¥: {e}")
    elif not HAS_TENSORBOARD:
        print_once("âš ï¸ è·³è¿‡TensorBoardè®°å½•ï¼ˆTensorBoardä¸å¯ç”¨ï¼‰")
    else:
        print_once("âš ï¸ è·³è¿‡TensorBoardè®°å½•ï¼ˆwriterä¸ºNoneï¼‰")
    
    print(f"âœ… éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
    
    return val_loss, val_acc


def save_checkpoint(model, optimizer, scaler, config, epoch, best_accuracy, checkpoint_dir, timestamp=None):
    """
    ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨å¥å£®çš„è·¯å¾„å¤„ç†å’Œå¼‚å¸¸å¤„ç†
    """
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨ - æ›´å¥å£®çš„å®ç°
    def safe_save_directory(dir_path):
        """å®‰å…¨åœ°ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨"""
        try:
            # è·å–çˆ¶ç›®å½•å¹¶ç¡®ä¿å­˜åœ¨
            parent_dir = os.path.dirname(dir_path)
            if parent_dir and not os.path.exists(parent_dir):
                os.makedirs(parent_dir, exist_ok=True)
                print(f"âœ… å·²åˆ›å»ºçˆ¶ç›®å½•: {parent_dir}")
            
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            os.makedirs(dir_path, exist_ok=True)
            print(f"âœ… ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨: {dir_path}")
            return True
        except Exception as e:
            print(f"âŒ åˆ›å»ºä¿å­˜ç›®å½•å¤±è´¥: {e}")
            return False
    
    # å¦‚æœæ²¡æœ‰æä¾›æ—¶é—´æˆ³ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ï¼ˆä»…ä½œä¸ºå¤‡ç”¨ï¼‰
    if timestamp is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # åˆ›å»ºåŸºäºæ—¶é—´æˆ³çš„æ¨¡å‹ä¿å­˜ç›®å½•
    model_dir = os.path.join('models', timestamp)
    
    # å°è¯•ä½¿ç”¨åŸºäºæ—¶é—´æˆ³çš„ç›®å½•
    if not safe_save_directory(model_dir):
        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨æä¾›çš„ç›®å½•
        if not safe_save_directory(checkpoint_dir):
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨å½“å‰ç›®å½•ä½œä¸ºå¤‡é€‰
            checkpoint_dir = os.path.dirname(os.path.abspath(__file__))
            print(f"âš ï¸ ä½¿ç”¨å½“å‰ç›®å½•ä½œä¸ºå¤‡é€‰: {checkpoint_dir}")
            safe_save_directory(checkpoint_dir)
        model_dir = checkpoint_dir
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹å­—å…¸
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scaler_state_dict': scaler.state_dict() if scaler else None,
        'best_accuracy': best_accuracy,
        'config': config
    }
    
    # å°è¯•ä¿å­˜æ¨¡å‹ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ–‡ä»¶å
    try:
        # ä½¿ç”¨ç®€å•çš„æ–‡ä»¶åï¼Œé¿å…è·¯å¾„é—®é¢˜
        checkpoint_filename = f'checkpoint_epoch_{epoch}.pth'
        checkpoint_path = os.path.join(model_dir, checkpoint_filename)
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        torch.save(checkpoint, checkpoint_path)
        print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        best_model_filename = 'best_model.pth'
        best_model_path = os.path.join(model_dir, best_model_filename)
        torch.save(model.state_dict(), best_model_path)
        print(f"ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_model_path}")
        
        # åŒæ—¶åœ¨æ ¹ç›®å½•ä¿å­˜ä¸€ä¸ªé“¾æ¥åˆ°æœ€æ–°çš„æœ€ä½³æ¨¡å‹
        root_best_model_path = os.path.join('models', 'best_model.pth')
        try:
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°è¯•åˆ é™¤
            if os.path.exists(root_best_model_path):
                os.remove(root_best_model_path)
            # åœ¨Windowsä¸Šï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶æ–‡ä»¶ä½œä¸ºæ›¿ä»£
            import shutil
            shutil.copy(best_model_path, root_best_model_path)
            print(f"ğŸ”— æœ€ä½³æ¨¡å‹é“¾æ¥å·²æ›´æ–°åˆ°: {root_best_model_path}")
        except Exception as e_link:
            print(f"âš ï¸ æ›´æ–°æœ€ä½³æ¨¡å‹é“¾æ¥å¤±è´¥: {e_link}")
        
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
        # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„è·¯å¾„å’Œæ–‡ä»¶å
        try:
            # åªä½¿ç”¨æ–‡ä»¶åï¼Œä¸åŒ…å«è·¯å¾„
            simple_checkpoint_path = f'checkpoint_epoch_{epoch}.pth'
            torch.save(checkpoint, simple_checkpoint_path)
            print(f"âš ï¸ ä½¿ç”¨ç®€å•è·¯å¾„ä¿å­˜æ£€æŸ¥ç‚¹: {simple_checkpoint_path}")
            
            simple_best_model_path = 'best_model.pth'
            torch.save(model.state_dict(), simple_best_model_path)
            print(f"âš ï¸ ä½¿ç”¨ç®€å•è·¯å¾„ä¿å­˜æœ€ä½³æ¨¡å‹: {simple_best_model_path}")
            return True
        except Exception as e2:
            print(f"âŒ ä½¿ç”¨ç®€å•è·¯å¾„ä¹Ÿä¿å­˜å¤±è´¥: {e2}")
            print("âš ï¸ è®­ç»ƒå®Œæˆä½†æ— æ³•ä¿å­˜æ¨¡å‹")
            return False


def load_checkpoint(checkpoint_path, model, optimizer, scaler):
    """
    åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
    """
    if not os.path.isfile(checkpoint_path):
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path)
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # åŠ è½½æ¢¯åº¦ç¼©æ”¾å™¨çŠ¶æ€
    if scaler and 'scaler_state_dict' in checkpoint and checkpoint['scaler_state_dict']:
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    print(f"ğŸ“‚ å·²ä» {checkpoint_path} åŠ è½½æ£€æŸ¥ç‚¹ï¼Œç»§ç»­ä»ç¬¬ {checkpoint['epoch']} è½®è®­ç»ƒ")
    
    return checkpoint['epoch'], checkpoint['best_accuracy'], checkpoint.get('config', {})


def setup_directories(save_dir, log_dir):
    """
    è®¾ç½®ä¿å­˜ç›®å½•
    """
    # æ£€æŸ¥å¹¶å¤„ç†æ¨¡å‹ä¿å­˜ç›®å½•
    if os.path.exists(save_dir):
        if not os.path.isdir(save_dir):
            print(f"è­¦å‘Š: {save_dir} ä¸æ˜¯ç›®å½•ï¼Œæ­£åœ¨åˆ é™¤å¹¶åˆ›å»ºæ–°ç›®å½•...")
            os.remove(save_dir)
    os.makedirs(save_dir, exist_ok=True)
    
    # æ£€æŸ¥å¹¶å¤„ç†æ—¥å¿—ç›®å½•
    if os.path.exists(log_dir):
        if not os.path.isdir(log_dir):
            print(f"è­¦å‘Š: {log_dir} ä¸æ˜¯ç›®å½•ï¼Œæ­£åœ¨åˆ é™¤å¹¶åˆ›å»ºæ–°ç›®å½•...")
            os.remove(log_dir)
    os.makedirs(log_dir, exist_ok=True)
    
    return save_dir, log_dir

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # åˆ›å»ºé…ç½®å­—å…¸
    config = {
        'model_name': args.model,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'lr': args.lr,
        'weight_decay': args.weight_decay,
        'val_ratio': args.val_ratio,
        'patience': args.patience,
        'save_dir': args.save_dir,
        'log_dir': args.log_dir,
        'resume': args.resume,
        'use_mixed_precision': args.use_mixed_precision,
        'gradient_accumulation_steps': args.gradient_accumulation_steps,
        'warmup_epochs': args.warmup_epochs,
        'use_cosine_lr_scheduler': args.use_cosine_lr_scheduler,
        # æ·»åŠ æ•°æ®é…ç½®
        'data_config': DEFAULT_DATA_CONFIG.copy()
    }
    config['data_config']['batch_size'] = config['batch_size']
    
    # ç”Ÿæˆå…¨å±€æ—¶é—´æˆ³ï¼Œç¡®ä¿æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ç›¸åŒçš„æ—¶é—´æˆ³æ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    print(f"ğŸ“‚ è®­ç»ƒæ—¶é—´æˆ³: {timestamp} (ç”¨äºç»„ç»‡æ¨¡å‹æ–‡ä»¶)")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ä½¿ç”¨ç»å¯¹è·¯å¾„å’Œè§„èŒƒåŒ–å¤„ç†
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºåŸºç¡€
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    # é…ç½®æ—¥å¿—ç›®å½•
    config['log_dir'] = os.path.abspath(os.path.join(base_dir, config['log_dir']))
    log_dir = config['log_dir']
    
    # é…ç½®ä¿å­˜ç›®å½•
    config['save_dir'] = os.path.abspath(os.path.join(base_dir, config['save_dir']))
    save_dir = config['save_dir']
    
    # åŠ å¼ºç›®å½•åˆ›å»ºé€»è¾‘ï¼Œç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
    def ensure_directory(directory_path):
        """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¤„ç†å„ç§å¯èƒ½çš„é”™è¯¯"""
        try:
            # è·å–çˆ¶ç›®å½•
            parent_dir = os.path.dirname(directory_path)
            if parent_dir and not os.path.exists(parent_dir):
                os.makedirs(parent_dir, exist_ok=True)
                print(f"âœ… å·²åˆ›å»ºçˆ¶ç›®å½•: {parent_dir}")
            
            # ç„¶ååˆ›å»ºç›®æ ‡ç›®å½•
            if os.path.exists(directory_path):
                if not os.path.isdir(directory_path):
                    print(f"âš ï¸ {directory_path} ä¸æ˜¯ç›®å½•ï¼Œæ­£åœ¨åˆ é™¤å¹¶é‡æ–°åˆ›å»º...")
                    try:
                        os.remove(directory_path)
                    except:
                        pass
            os.makedirs(directory_path, exist_ok=True)
            print(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {directory_path}")
            return True
        except Exception as e:
            print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    if not ensure_directory(log_dir):
        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•
        import tempfile
        log_dir = tempfile.mkdtemp(prefix='mnist_logs_')
        config['log_dir'] = log_dir
        print(f"âš ï¸ åˆ‡æ¢åˆ°ä¸´æ—¶æ—¥å¿—ç›®å½•: {log_dir}")
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    if not ensure_directory(save_dir):
        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨å½“å‰ç›®å½•
        save_dir = base_dir
        config['save_dir'] = save_dir
        print(f"âš ï¸ åˆ‡æ¢åˆ°å½“å‰ç›®å½•ä½œä¸ºä¿å­˜è·¯å¾„: {save_dir}")
    
    # åˆ›å»ºTensorBoardæ—¥å¿—è®°å½•å™¨
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    tb_log_dir = os.path.join(log_dir, f'{config["model_name"]}_{timestamp}')
    
    # ä¸è¿›è¡Œè·¯å¾„åˆ†éš”ç¬¦æ›¿æ¢ï¼Œä½¿ç”¨åŸç”ŸWindowsè·¯å¾„
    
    # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•åˆ›å»ºåµŒå¥—ç›®å½•
    # å…ˆåˆ é™¤å¯èƒ½å­˜åœ¨çš„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    if os.path.exists(tb_log_dir) and not os.path.isdir(tb_log_dir):
        print(f"è­¦å‘Š: {tb_log_dir} å­˜åœ¨ä½†ä¸æ˜¯ç›®å½•ï¼Œæ­£åœ¨åˆ é™¤...")
        os.remove(tb_log_dir)
    
    # ä½¿ç”¨osæ¨¡å—åˆ›å»ºç›®å½•ï¼Œè€Œä¸æ˜¯ä¾èµ–TensorFlow
    try:
        # åˆ†æ­¥åˆ›å»ºç›®å½•ï¼Œç¡®ä¿æ¯ä¸€å±‚éƒ½æ˜¯ç›®å½•
        parts = tb_log_dir.split(os.sep)
        current_path = parts[0] + os.sep if os.name == 'nt' else parts[0]
        
        for part in parts[1:]:
            current_path = os.path.join(current_path, part)
            if os.path.exists(current_path):
                if not os.path.isdir(current_path):
                    print(f"è­¦å‘Š: {current_path} ä¸æ˜¯ç›®å½•ï¼Œæ­£åœ¨åˆ é™¤...")
                    os.remove(current_path)
            if not os.path.exists(current_path):
                os.makedirs(current_path, exist_ok=True)
        
        print(f"âœ… å·²æˆåŠŸåˆ›å»ºæ—¥å¿—ç›®å½•: {tb_log_dir}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ—¥å¿—ç›®å½•æ—¶å‡ºé”™: {e}")
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä¸´æ—¶ç›®å½•
        import tempfile
        tb_log_dir = tempfile.mkdtemp(prefix=f"mnist_{config['model_name']}_")
        # ä½¿ç”¨åŸç”ŸWindowsè·¯å¾„æ ¼å¼
        print(f"âš ï¸ ä½¿ç”¨ä¸´æ—¶ç›®å½•æ›¿ä»£: {tb_log_dir}")
    
    # å†æ¬¡ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œé¿å…åˆ›å»ºwriteræ—¶å‡ºé”™
    try:
        os.makedirs(tb_log_dir, exist_ok=True)
        print(f"ğŸ“ å†æ¬¡ç¡®è®¤å¹¶åˆ›å»ºæ—¥å¿—ç›®å½•: {tb_log_dir}")
    except Exception as e:
        print(f"âŒ å†æ¬¡åˆ›å»ºæ—¥å¿—ç›®å½•å¤±è´¥: {e}")
        # ä½¿ç”¨æ›´ç®€å•çš„ç›®å½•è·¯å¾„ä½œä¸ºå¤‡é€‰
        tb_log_dir = os.path.join(log_dir, f'tb_{config["model_name"]}')
        try:
            os.makedirs(tb_log_dir, exist_ok=True)
            print(f"âš ï¸ ä½¿ç”¨ç®€åŒ–çš„æ—¥å¿—ç›®å½•è·¯å¾„: {tb_log_dir}")
        except Exception as e2:
            print(f"âŒ åˆ›å»ºç®€åŒ–æ—¥å¿—ç›®å½•ä¹Ÿå¤±è´¥: {e2}")
    
    # å°è¯•ç›´æ¥åˆå§‹åŒ–SummaryWriter
    writer = None
    if HAS_TENSORBOARD:
        try:
            # ä¸ä½¿ç”¨ä¸­æ–‡è·¯å¾„ï¼Œé¿å…ç¼–ç é—®é¢˜
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„å¯èƒ½æ›´å¯é 
            if os.name == 'nt':  # Windowsç³»ç»Ÿ
                # å°è¯•ä½¿ç”¨çº¯ASCIIå­—ç¬¦çš„è·¯å¾„
                tb_log_dir_safe = os.path.join(log_dir, f'tb_{config["model_name"]}_{timestamp}')
                os.makedirs(tb_log_dir_safe, exist_ok=True)
                writer = SummaryWriter(log_dir=tb_log_dir_safe)
                print(f"âœ… TensorBoardæ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–æˆåŠŸ (ä½¿ç”¨å®‰å…¨è·¯å¾„: {tb_log_dir_safe})")
            else:
                writer = SummaryWriter(log_dir=tb_log_dir)
                print("âœ… TensorBoardæ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ TensorBoardåˆå§‹åŒ–å¤±è´¥: {e}")
            # å¦‚æœSummaryWriterå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•
            try:
                # å°è¯•ä½¿ç”¨æœ€åŸºç¡€çš„è·¯å¾„
                simple_log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')
                os.makedirs(simple_log_dir, exist_ok=True)
                writer = SummaryWriter(log_dir=simple_log_dir)
                print(f"âš ï¸ å°è¯•ä½¿ç”¨ç®€å•è·¯å¾„åˆå§‹åŒ–TensorBoard: {simple_log_dir}")
            except Exception as e2:
                print(f"âŒ ä½¿ç”¨ç®€å•è·¯å¾„ä¹Ÿå¤±è´¥: {e2}")
                print("âš ï¸ è·³è¿‡TensorBoardåˆå§‹åŒ–ï¼Œç»§ç»­è®­ç»ƒ")
                writer = None
    else:
        print("â„¹ï¸ TensorBoardä¸å¯ç”¨ï¼Œè·³è¿‡åˆ›å»ºwriter")
    
    # ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶ - å¢å¼ºçš„å¼‚å¸¸å¤„ç†
    try:
        config_save_path = os.path.join(log_dir, f'{config["model_name"]}_{timestamp}_config.json')
        with open(config_save_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“‹ é…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")
        # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ–‡ä»¶å
        try:
            simple_config_path = os.path.join(log_dir, 'config.json')
            with open(simple_config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"âš ï¸ å·²ä½¿ç”¨ç®€åŒ–åç§°ä¿å­˜é…ç½®: {simple_config_path}")
        except Exception as e2:
            print(f"âŒ é…ç½®ä¿å­˜å®Œå…¨å¤±è´¥: {e2}")
            print("âš ï¸ ç»§ç»­è®­ç»ƒï¼Œä½†ä¸ä¼šä¿å­˜é…ç½®")
    
    # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ - å¢å¼ºçš„é”™è¯¯å¤„ç†
    train_loader = None
    val_loader = None
    print("ğŸ“¥ æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    
    # å°è¯•åŠ è½½æ•°æ®é›†ï¼Œæœ€å¤šå°è¯•3æ¬¡
    max_data_load_attempts = 3
    for attempt in range(max_data_load_attempts):
        try:
            data_loader = MNISTDataLoader(config['data_config'])
            train_loader, val_loader = data_loader.get_train_val_loaders(config['val_ratio'])
            # éªŒè¯æ•°æ®åŠ è½½å™¨
            if train_loader is not None and val_loader is not None:
                print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}, éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
                break
            else:
                raise ValueError("æ•°æ®åŠ è½½å™¨è¿”å›None")
        except Exception as e:
            print(f"âŒ ç¬¬ {attempt+1}/{max_data_load_attempts} æ¬¡åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            # è°ƒæ•´é…ç½®é‡è¯•
            if attempt < max_data_load_attempts - 1:
                # å‡å°æ‰¹é‡å¤§å°é‡è¯•
                if config['batch_size'] > 8:
                    config['batch_size'] = max(8, config['batch_size'] // 2)
                    config['data_config']['batch_size'] = config['batch_size']
                    print(f"âš ï¸ å‡å°æ‰¹é‡å¤§å°è‡³ {config['batch_size']} å¹¶é‡è¯•...")
                time.sleep(1)
    
    if train_loader is None or val_loader is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®é›†ï¼Œç¨‹åºç»ˆæ­¢")
        sys.exit(1)
    
    # åˆå§‹åŒ–æ¨¡å‹ - å¢å¼ºçš„é”™è¯¯å¤„ç†
    model = None
    print("ğŸ—ï¸ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    
    # å°è¯•åŠ è½½æ¨¡å‹ï¼Œæä¾›å¤‡é€‰æ–¹æ¡ˆ
    model_attempts = [
        (config['model_name'], "æŒ‡å®šæ¨¡å‹"),
        ('SimpleCNN', "ç®€å•CNNå¤‡é€‰"),
        ('ResNet18', "ResNet18å¤‡é€‰")
    ]
    
    for model_name, model_desc in model_attempts:
        try:
            print(f"å°è¯•åŠ è½½{model_desc}: {model_name}")
            model = get_model(model_name)
            if model is None:
                raise ValueError(f"get_modelè¿”å›None: {model_name}")
            
            # å°è¯•ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
            try:
                model = model.to(device)
                print(f"âœ… {model_name} æˆåŠŸåŠ è½½å¹¶ç§»è‡³ {device}")
                # æ›´æ–°é…ç½®ä»¥åæ˜ å®é™…ä½¿ç”¨çš„æ¨¡å‹
                config['model_name'] = model_name
                break
            except Exception as e:
                print(f"âŒ æ— æ³•ç§»åŠ¨{model_name}åˆ°{device}: {e}")
                # å°è¯•åœ¨CPUä¸ŠåŠ è½½
                if device.type == 'cuda':
                    print("âš ï¸ å°è¯•åœ¨CPUä¸ŠåŠ è½½æ¨¡å‹...")
                    model = model.to('cpu')
                    device = torch.device('cpu')
                    print(f"âœ… {model_name} æˆåŠŸåŠ è½½åˆ°CPU")
                    config['model_name'] = model_name
                    break
        except Exception as e:
            print(f"âŒ åŠ è½½{model_desc}å¤±è´¥: {e}")
    
    if model is None:
        print("âŒ æ— æ³•åŠ è½½ä»»ä½•æ¨¡å‹ï¼Œç¨‹åºç»ˆæ­¢")
        sys.exit(1)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"ğŸ“Š æ¨¡å‹: {config['model_name']}")
    print(f"ğŸ§® å‚æ•°æ•°é‡: {count_parameters(model):,}")
    
    # åˆå§‹åŒ–æŸå¤±å‡½æ•° - å®‰å…¨åˆå§‹åŒ–
    criterion = None
    try:
        criterion = nn.CrossEntropyLoss()
        criterion = criterion.to(device)
        print("âœ… æŸå¤±å‡½æ•°åˆå§‹åŒ–æˆåŠŸ (CrossEntropyLoss)")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–æŸå¤±å‡½æ•°å¤±è´¥: {e}")
        print("âš ï¸ å°è¯•ä½¿ç”¨å¤‡é€‰æŸå¤±å‡½æ•°...")
        # å°è¯•å¤‡é€‰æŸå¤±å‡½æ•°
        alternative_losses = [
            (nn.NLLLoss(), "NLLLoss"),
            (nn.MSELoss(), "MSELoss")
        ]
        for alt_criterion, name in alternative_losses:
            try:
                criterion = alt_criterion
                criterion = criterion.to(device)
                print(f"âœ… æˆåŠŸä½¿ç”¨å¤‡é€‰æŸå¤±å‡½æ•°: {name}")
                break
            except Exception as e2:
                print(f"âŒ å¤‡é€‰æŸå¤±å‡½æ•° {name} åˆå§‹åŒ–å¤±è´¥: {e2}")
    
    if criterion is None:
        print("âŒ æ— æ³•åˆå§‹åŒ–ä»»ä½•æŸå¤±å‡½æ•°ï¼Œç¨‹åºç»ˆæ­¢")
        sys.exit(1)
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = create_optimizer(model, config['lr'], config['weight_decay'])
    
    # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾å™¨ - å®‰å…¨åˆå§‹åŒ–
    scaler = None
    if config['use_mixed_precision']:
        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨CUDAä¸Šè¿è¡Œ
            if device.type == 'cuda':
                scaler = torch.cuda.amp.GradScaler()
                print("âœ… æ¢¯åº¦ç¼©æ”¾å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                raise ValueError("æ··åˆç²¾åº¦è®­ç»ƒä»…æ”¯æŒCUDA")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–æ¢¯åº¦ç¼©æ”¾å™¨å¤±è´¥: {e}")
            print("âš ï¸ ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
            config['use_mixed_precision'] = False
    
    # è®¡ç®—æ€»æ­¥æ•°ï¼ˆç”¨äºä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼‰- å®‰å…¨è®¡ç®—
    try:
        total_steps = config['epochs'] * len(train_loader)
        if total_steps <= 0:
            raise ValueError("æ€»æ­¥æ•°å¿…é¡»å¤§äº0")
        print(f"ğŸ“ˆ æ€»è®­ç»ƒæ­¥æ•°: {total_steps:,}")
    except Exception as e:
        print(f"âŒ è®¡ç®—æ€»æ­¥æ•°å¤±è´¥: {e}")
        # è®¾ç½®åˆç†çš„é»˜è®¤å€¼
        total_steps = 1000
        print(f"âš ï¸ ä½¿ç”¨é»˜è®¤æ€»æ­¥æ•°: {total_steps}")
    
    # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨ - å¢å¼ºçš„å¼‚å¸¸å¤„ç†
    scheduler = None
    try:
        scheduler = create_lr_scheduler(optimizer, config, total_steps)
        print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨å¤±è´¥: {e}")
        print("âš ï¸ ä½¿ç”¨ç®€å•çš„é˜¶æ¢¯å¼è°ƒåº¦å™¨ä½œä¸ºå¤‡é€‰")
        
        # å°è¯•å¤šç§å¤‡é€‰è°ƒåº¦å™¨
        try:
            # é˜¶æ¢¯å¼è°ƒåº¦å™¨
            step_size = max(1, config['epochs'] // 3)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)
            print(f"âœ… æˆåŠŸä½¿ç”¨StepLRè°ƒåº¦å™¨ (step_size={step_size})")
        except Exception as e2:
            print(f"âŒ å¤‡é€‰è°ƒåº¦å™¨ä¹Ÿå¤±è´¥: {e2}")
            print("âš ï¸ å°†ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")
            # åˆ›å»ºä¸€ä¸ªä¸åšä»»ä½•æ“ä½œçš„è°ƒåº¦å™¨
            class NoOpScheduler:
                def step(self):
                    pass
            scheduler = NoOpScheduler()
    
    # è®¾ç½®æ—©åœå‚æ•°
    best_accuracy = 0.0
    epochs_no_improve = 0
    
    # åˆå§‹åŒ–è®­ç»ƒå†å²è®°å½•
    training_history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': []
    }
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
    start_epoch = 1
    if config['resume']:
        start_epoch, best_accuracy, loaded_config = load_checkpoint(
            config['resume'], model, optimizer, scaler
        )
        # æ›´æ–°é…ç½®ï¼ˆå¦‚æœåŠ è½½çš„é…ç½®ä¸å½“å‰é…ç½®ä¸åŒï¼‰
        config.update(loaded_config)
    
    print(f"\nğŸ”§ è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        if key != 'data_config':
            print(f"  {key}: {value}")
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ ({'æ··åˆç²¾åº¦' if config['use_mixed_precision'] else 'å•ç²¾åº¦'})")
    start_time = time.time()
    
    try:
        # è®­ç»ƒå¾ªç¯ - å¢å¼ºçš„é”™è¯¯æ¢å¤èƒ½åŠ›
        for epoch in range(start_epoch, config['epochs'] + 1):
            try:
                print(f"\nğŸ”„ å¼€å§‹è½®æ¬¡ {epoch}/{config['epochs']} - å­¦ä¹ ç‡: {get_lr(optimizer):.6f}")
                
                # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                try:
                    model = model.to(device)
                except:
                    pass
                
                # è®­ç»ƒä¸€ä¸ªè½®æ¬¡ - å¼‚å¸¸å¤„ç†
                train_loss, train_acc = None, None
                try:
                    train_loss, train_acc = train_one_epoch(
                        model, train_loader, criterion, optimizer, scaler, config, device, epoch, writer
                    )
                    if train_loss is None or train_acc is None:
                        raise ValueError("è®­ç»ƒå‡½æ•°è¿”å›Noneå€¼")
                except KeyboardInterrupt:
                    raise  # é‡æ–°æŠ›å‡ºä»¥åœ¨å¤–éƒ¨æ•è·
                except Exception as e:
                    print(f"âŒ è®­ç»ƒè½®æ¬¡å¤±è´¥: {e}")
                    # å°è¯•æ¢å¤è®­ç»ƒçŠ¶æ€
                    try:
                        # é‡ç½®æ¢¯åº¦
                        optimizer.zero_grad(set_to_none=True)
                        # é™ä½å­¦ä¹ ç‡
                        for param_group in optimizer.param_groups:
                            param_group['lr'] *= 0.5
                        print(f"âš ï¸ å·²é™ä½å­¦ä¹ ç‡è‡³: {get_lr(optimizer):.6f}")
                    except:
                        pass
                    # è·³è¿‡å½“å‰è½®æ¬¡çš„éªŒè¯
                    continue
                
                # éªŒè¯ - å¼‚å¸¸å¤„ç†
                val_loss, val_acc = None, None
                try:
                    val_loss, val_acc = validate(model, val_loader, criterion, device, epoch, writer)
                    if val_loss is None or val_acc is None:
                        raise ValueError("éªŒè¯å‡½æ•°è¿”å›Noneå€¼")
                except Exception as e:
                    print(f"âŒ éªŒè¯å¤±è´¥: {e}")
                    # ä½¿ç”¨é»˜è®¤å€¼ä½œä¸ºå¤‡é€‰
                    val_acc = best_accuracy * 0.99  # ä½¿ç”¨ç¨ä½äºæœ€ä½³çš„å‡†ç¡®ç‡
                    val_loss = float('inf')
                    print(f"âš ï¸ ä½¿ç”¨å¤‡é€‰éªŒè¯ç»“æœ")
                
                # æ›´æ–°å­¦ä¹ ç‡ - å®‰å…¨æ£€æŸ¥
                try:
                    if scheduler is not None:
                        scheduler.step()
                except Exception as e:
                    print(f"âŒ æ›´æ–°å­¦ä¹ ç‡å¤±è´¥: {e}")
                    print("âš ï¸ ç»§ç»­ä½¿ç”¨å½“å‰å­¦ä¹ ç‡")
                
                # è®°å½•è®­ç»ƒå†å²
                training_history['train_loss'].append(train_loss)
                training_history['train_acc'].append(train_acc)
                training_history['val_loss'].append(val_loss)
                training_history['val_acc'].append(val_acc)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                is_best = False
                try:
                    is_best = val_acc > best_accuracy
                except:
                    is_best = False
                
                if is_best:
                    best_accuracy = val_acc
                    epochs_no_improve = 0
                    # ä¿å­˜æ£€æŸ¥ç‚¹å’Œæœ€ä½³æ¨¡å‹ - å¼‚æ­¥ä¿å­˜å‡å°‘è®­ç»ƒä¸­æ–­
                    try:
                        save_success = save_checkpoint(model, optimizer, scaler, config, epoch, best_accuracy, save_dir, timestamp)
                        if not save_success:
                            print("âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥ï¼Œä½†è®­ç»ƒç»§ç»­")
                    except Exception as e:
                        print(f"âŒ ä¿å­˜æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                else:
                    epochs_no_improve += 1
                    print(f"â³ å·² {epochs_no_improve} è½®æ²¡æœ‰æ”¹è¿›ï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")
                
                # æ—©åœæ£€æŸ¥
                if epochs_no_improve >= config['patience']:
                    print(f"ğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶ï¼è®­ç»ƒæå‰ç»“æŸã€‚")
                    break
                
                print()  # ç©ºè¡Œåˆ†éš”è½®æ¬¡
                
            except KeyboardInterrupt:
                raise  # é‡æ–°æŠ›å‡ºä»¥åœ¨å¤–éƒ¨æ•è·
            except Exception as e:
                print(f"âŒ è½®æ¬¡ {epoch} å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
                # å°è¯•é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€
                try:
                    # é™ä½å­¦ä¹ ç‡
                    for param_group in optimizer.param_groups:
                        param_group['lr'] *= 0.5
                    print(f"âš ï¸ å·²é‡ç½®å­¦ä¹ ç‡ä¸º: {get_lr(optimizer):.6f}")
                    # æ¸…ç†æ¢¯åº¦
                    optimizer.zero_grad(set_to_none=True)
                except Exception as reset_error:
                    print(f"âŒ å°è¯•æ¢å¤è®­ç»ƒçŠ¶æ€å¤±è´¥: {reset_error}")
                    # è€ƒè™‘æ—©åœä»¥é¿å…æ— é™é”™è¯¯å¾ªç¯
                    if epoch > start_epoch + 5:  # åªåœ¨è®­ç»ƒäº†å‡ è½®åæ‰è€ƒè™‘æ—©åœ
                        print("âš ï¸ è¿ç»­é”™è¯¯ï¼Œè€ƒè™‘æå‰ç»ˆæ­¢è®­ç»ƒ")
                        break
        
    except KeyboardInterrupt:
        print("\nâ¸ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    finally:
        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_time = time.time() - start_time
        hours, rem = divmod(total_time, 3600)
        minutes, seconds = divmod(rem, 60)
        
        print(f"\nğŸ è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {seconds:.1f}ç§’")
        print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.2f}%")
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {os.path.join(save_dir, 'best_model.pth')}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        try:
            # ç¡®ä¿logsç›®å½•å­˜åœ¨
            os.makedirs('logs', exist_ok=True)
            # ä¿å­˜è®­ç»ƒå†å²åˆ°npyæ–‡ä»¶
            history_filename = f'logs/{config["model_name"]}_{timestamp}_training_history.npy'
            np.save(history_filename, training_history)
            print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {history_filename}")
            
            # åŒæ—¶ä¿å­˜ä¸€ä¸ªé€šç”¨è·¯å¾„çš„å†å²æ–‡ä»¶
            general_history_path = 'logs/training_history.npy'
            np.save(general_history_path, training_history)
            print(f"ğŸ“Š é€šç”¨è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {general_history_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜è®­ç»ƒå†å²å¤±è´¥: {e}")
            
        # å…³é—­TensorBoardæ—¥å¿—è®°å½•å™¨
        if writer is not None and HAS_TENSORBOARD:
            try:
                writer.close()
                print_once("âœ… TensorBoard writerå·²å…³é—­")
            except Exception as e:
                print_once(f"âš ï¸ å…³é—­TensorBoard writeræ—¶å‡ºé”™: {e}")
        elif not HAS_TENSORBOARD:
            print_once("âš ï¸ è·³è¿‡å…³é—­TensorBoard writerï¼ˆTensorBoardä¸å¯ç”¨ï¼‰")
        else:
            print_once("âš ï¸ è·³è¿‡å…³é—­TensorBoard writerï¼ˆwriterä¸ºNoneï¼‰")


if __name__ == "__main__":
    main()